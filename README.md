# Project--Hadoop-41
# Project--Hadoop-40(Challenges)

<table>
  
**In this project We will use Spark with Python to do an amazing stuff.Here we will work on the Spark DataFrame which will read the csv files attached above as our dataset and complete the challenges as provided by Jose Portilla .** <br></br>

**Here is the Problem Statement!** <br></br>

A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? **Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.** <br></br>
Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet).<br></br>

**Before jumping to the code lets understand Spark and Logistic Regression First**...<br></br>

**What is Apache Spark?** <br></br>

Apache Spark‚Ñ¢ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.<br></br>
Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R.
Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses.<br></br>
Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling.
Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.<br></br>

**What is Spark DataFrames**?<br></br>

**1**: Spark 2.0 shifted towards DataFrame syntax<br></br>
**2**: are now the standard way of using Spark's ML Capabilties<br></br>
**3**: Spark Docs are still new<br></br>
**4**: DataFrame is very familiar to Pandas DataFrames<br></br>
**5**: Columns = features<br></br>
**6**: Rows = records<br></br>

**What is Logistic Regression**?<br></br>
Logistic regression is a statistical method that is used for building machine learning models where the dependent variable is dichotomous: i.e. binary. Logistic regression is used to describe data and the relationship between one dependent variable and one or more independent variables.<br></br>


**Important Note: Go through the customer_churn.csv and new_customers.csv files before jumping to the code.**


</table>

**So what are you waiting for...? Jump to the code to get started. As usual for any doubt or query see you in pull request section üòÅüòÇ. Thanks!**


